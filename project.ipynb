{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Future/CSC413-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CURRENT PREGRESS AND NOTES:\n",
        "\n",
        "It takes 10 sec to run 1 epoch on colab\n",
        "\n",
        "After 100 epochs of training the model performs much better. Problems is, in the area where it's supposed to be completely black or white, the model outputs opposite color pixel.\n",
        "\n",
        "Maybe try Adam rather than SGD\n",
        "\n",
        "I forgot to save the work after training for 100 epochs"
      ],
      "metadata": {
        "id": "NSa60u7DEBNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Download"
      ],
      "metadata": {
        "id": "noRLrY17zw2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "1YfLyw9jHVIa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import torch\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 67M\n",
        "!curl --remote-name -L https://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\n",
        "!tar xzf BSR_bsds500.tgz\n",
        "!rm BSR_bsds500.tgz\n",
        "!mv BSR/BSDS500/data/images/* BSR/BSDS500\n",
        "!rm -r BSR/bench\n",
        "!rm -r BSR/documentation\n",
        "!rm -r BSR/BSDS500/data\n",
        "\n",
        "# Move all samples in test folder to train folder\n",
        "# So that we have 400 training examples\n",
        "# %mv BSR/BSDS500/data/images/test/*.jpg BSR/BSDS500/data/images/train\n",
        "\n",
        "# 428M\n",
        "!curl --remote-name -L https://image-net.org/data/ILSVRC/2017/ILSVRC2017_DET_test_new.tar.gz\n",
        "!tar xzf ILSVRC2017_DET_test_new.tar.gz\n",
        "!rm ILSVRC2017_DET_test_new.tar.gz\n",
        "!mv ILSVRC/Data/DET/test/*.JPEG ILSVRC\n",
        "!rm -r ILSVRC/Data\n",
        "!rm -r ILSVRC/ImageSets"
      ],
      "metadata": {
        "id": "kjG9_1zyJ5F_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869078ff-7b7c-457b-d746-d2a51bb30787"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 67.4M  100 67.4M    0     0  12.2M      0  0:00:05  0:00:05 --:--:-- 14.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  427M  100  427M    0     0  4029k      0  0:01:48  0:01:48 --:--:-- 16.5M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "HWWb-80qDm_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "metadata": {
        "id": "vqQUEaUMzYUA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "class BSDS500Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split, scale, resize):\n",
        "        self.root = \"BSR/BSDS500\"\n",
        "        self.split = split # train / test / val\n",
        "        self.scale = scale\n",
        "        self.resize = resize\n",
        "        self.low_res_size = self.resize // self.scale\n",
        "        self.high_res_size = self.low_res_size * self.scale\n",
        "        self.file_names = [os.path.join(self.root, self.split, file_name) for file_name in os.listdir(os.path.join(self.root, self.split)) if file_name.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Convert to YUV and split the channels\n",
        "        image = Image.open(self.file_names[index]).convert(\"YCbCr\")\n",
        "        y, u, v = image.split()\n",
        "\n",
        "        # Rescale images to square\n",
        "        low_composed = Compose([\n",
        "            Resize([self.low_res_size, self.low_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = Compose([\n",
        "            Resize([self.high_res_size, self.high_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        return { \"y_low\": low_composed(y).cuda(), \"u_low\": low_composed(u).cuda(), \"v_low\": low_composed(v).cuda(),\n",
        "               \"y_high\": high_composed(y).cuda(), \"u_high\": high_composed(u).cuda(), \"v_high\": high_composed(v).cuda(),\n",
        "               \"high\": high_composed(image).cuda() }\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "syvEe3KfDm_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ILSVRC2017Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, scale: int, resize: int):\n",
        "        self.root = \"ILSVRC\"\n",
        "        self.scale = scale\n",
        "        self.resize = resize\n",
        "        self.low_res_size = self.resize // self.scale\n",
        "        self.high_res_size = self.low_res_size * self.scale\n",
        "        self.file_names = [os.path.join(self.root, file_name) for file_name in os.listdir(self.root) if file_name.endswith(\".JPEG\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Convert to YUV and split the channels\n",
        "        image = Image.open(self.file_names[index]).convert(\"YCbCr\")\n",
        "        y, u, v = image.split()\n",
        "\n",
        "        # Rescale images to square\n",
        "        low_composed = Compose([\n",
        "            Resize([self.low_res_size, self.low_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = Compose([\n",
        "            Resize([self.high_res_size, self.high_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        return { \"y_low\": low_composed(y).cuda(), \"u_low\": low_composed(u).cuda(), \"v_low\": low_composed(v).cuda(),\n",
        "               \"y_high\": high_composed(y).cuda(), \"u_high\": high_composed(u).cuda(), \"v_high\": high_composed(v).cuda(),\n",
        "               \"high\": high_composed(image).cuda() }"
      ],
      "metadata": {
        "id": "16MNn2Ms5SSo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate PSNR between two images (tensor)\n",
        "def psnr(img_1, img_2):\n",
        "    mse = torch.mean((img_1 - img_2) ** 2)\n",
        "    if(mse == 0):\n",
        "        return 100\n",
        "    max_pixel = 1\n",
        "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
        "    return psnr"
      ],
      "metadata": {
        "id": "pz-10hTqhkS3"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESPCN"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "a0Pf5rbtDm_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.xavier_uniform(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "class ESPCN_model(nn.Module):\n",
        "    def __init__(self, layer1_channel, layer2_channel, upscale_factor):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, layer1_channel, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(layer1_channel, layer2_channel, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(layer2_channel, upscale_factor ** 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(upscale_factor) # This function is literally built for ESPCN\n",
        "        )\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        return out"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vQzDVgEjDm_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet"
      ],
      "metadata": {
        "id": "ODWynRpJP3mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From PA2\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours, num_in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ############### \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(num_in_channels, num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, 2 * num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(2 * num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * num_filters, num_filters, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(num_filters + num_filters, num_colours, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_colours),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.output = nn.Conv2d(num_in_channels + num_colours, num_colours, kernel, padding=padding)\n",
        "        ###################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        a = self.block1(x)\n",
        "        b = self.block2(a)\n",
        "        c = self.block3(b)\n",
        "        c1 = torch.cat((a, c), dim=1)\n",
        "        d = self.block4(c1)\n",
        "        d1 = torch.cat((x, d), dim=1)\n",
        "        return self.output(d1)\n",
        "        ###################################################"
      ],
      "metadata": {
        "id": "0fE-nxSEP5Nw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ttXN0H-QP7aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for validation"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "G9anRABkDm_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "outputs": [],
      "source": [
        "def run_validation_step(\n",
        "    cnn,\n",
        "    criterion,\n",
        "    val_set,\n",
        "):\n",
        "    losses = []\n",
        "    psnrs = []\n",
        "\n",
        "    for i_batch, sample_batched in enumerate(val_set):\n",
        "          input = sample_batched[\"y_low\"]\n",
        "          output = cnn(input)\n",
        "\n",
        "          loss = criterion(output, sample_batched[\"y_high\"])\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "          # Me trying to use psnr in tf\n",
        "          # input_numpy = convert_to_tensor(input.squeeze().cpu().detach().numpy())\n",
        "          # output_numpy = convert_to_tensor(output.squeeze().cpu().detach().numpy())\n",
        "\n",
        "          # regular_upscale = Resize([args.resize, args.resize], InterpolationMode.BICUBIC)(input)\n",
        "\n",
        "          psnrs.append(psnr(output, sample_batched[\"y_high\"]))\n",
        "\n",
        "    val_loss = np.mean(losses)\n",
        "    val_psnr = np.mean(psnrs)\n",
        "    return val_loss, val_psnr"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Zv739LcHDm_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for training (adapted from PA2)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "qtDyptH0Dm_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "outputs": [],
      "source": [
        "def train(args, cnn=None):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # torch.set_num_threads(5)\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.model\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        cnn = ESPCN_model(64, 32, 3)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    if args.optimizor is \"SGD\":\n",
        "      optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4, nesterov=False)\n",
        "    elif args.optimizor is \"Adam\":\n",
        "      optimizer = torch.optim.Adam(cnn.parameters())\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    if args.train_set is \"ImageNet\":\n",
        "      trainset = torch.utils.data.DataLoader(ILSVRC2017Dataset(scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "    elif args.train_set is \"BSDS\":\n",
        "      trainset = torch.utils.data.DataLoader(BSDS500Dataset(split=\"train\", scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "\n",
        "    testset = torch.utils.data.DataLoader(BSDS500Dataset(split=\"val\", scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "\n",
        "    # for item in trainset:\n",
        "    #   print(item)\n",
        "\n",
        "    # y.save(\"temp.jpg\", \"JPEG\")\n",
        "\n",
        "    # print(\"Transforming data...\")\n",
        "    # train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    # train_rgb_cat = get_rgb_cat(train_rgb, colours)\n",
        "    # test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    # test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_PSNR = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "\n",
        "        for i_batch, sample_batched in enumerate(trainset):\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          input = sample_batched[\"y_low\"]\n",
        "          out = cnn(input)\n",
        "\n",
        "          # print(out.size())\n",
        "          # print(sample_batched[\"y_high\"].size())\n",
        "\n",
        "          loss = criterion(out, sample_batched[\"y_high\"])\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "        # plot training images\n",
        "        avg_loss = np.mean(losses)\n",
        "        train_losses.append(avg_loss)\n",
        "        time_elapsed = time.time() - start\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Train Loss: %.4f, Time (s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        # if epoch % 5 == 0:\n",
        "        cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "\n",
        "        val_loss, val_psnr = run_validation_step(cnn, criterion, testset)\n",
        "\n",
        "        time_elapsed = time.time() - start\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_PSNR.append(val_psnr)\n",
        "\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Val Loss: %.4f, Val PSNR: %.4f, Time(s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, val_loss, val_psnr, time_elapsed)\n",
        "          )\n",
        "        # else:\n",
        "        #   valid_losses.append(None)\n",
        "\n",
        "    # Plot training curve\n",
        "    plot1 = plt.figure(1)\n",
        "    plt.plot(train_losses, \"ro-\", label=\"Train\")\n",
        "    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/training_curve.png\")\n",
        "\n",
        "    plot2 = plt.figure(2)\n",
        "    plt.plot(valid_PSNR, \"yo-\", label=\"PSNR\")\n",
        "    plt.legend()\n",
        "    plt.title(\"PSNR\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/PSNR_curve.png\")\n",
        "\n",
        "    if args.checkpoint:\n",
        "        print(\"Saving model...\")\n",
        "        torch.save(cnn.state_dict(), args.checkpoint)\n",
        "\n",
        "    return cnn"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dPEllswFDm_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Oru2s9VqDm_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Beginning training ...\n",
            "Epoch [1/50], Train Loss: 0.0171, Time (s): 134.66\n",
            "Epoch [1/50], Val Loss: 0.0049, Val PSNR: 23.1073, Time(s): 136.88\n",
            "Epoch [2/50], Train Loss: 0.0053, Time (s): 271.66\n",
            "Epoch [2/50], Val Loss: 0.0033, Val PSNR: 24.8678, Time(s): 273.91\n",
            "Epoch [3/50], Train Loss: 0.0040, Time (s): 409.13\n",
            "Epoch [3/50], Val Loss: 0.0028, Val PSNR: 25.5822, Time(s): 411.37\n"
          ]
        }
      ],
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"model.pkl\",\n",
        "    \"model\": \"ESPCN\",\n",
        "    \"scale\": 3,\n",
        "    \"resize\": 480,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 50,\n",
        "    \"seed\": 42,\n",
        "    \"optimizor\": \"Adam\",\n",
        "    \"train_set\": \"ImageNet\"\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pny-9IhwDm_S",
        "outputId": "027f37c0-0f1e-4684-fc82-fe6f02fc6b74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BRI8IDUmDm_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "val_set = BSDS500Dataset(split=\"val\", scale=3, resize=481)\n",
        "\n",
        "index = 0\n",
        "cnn.eval()\n",
        "for item in val_set:\n",
        "\n",
        "  if index == 50:\n",
        "    img_arr = item[\"y_low\"]\n",
        "    img = ToPILImage()(img_arr)\n",
        "    img.save(\"in.jpg\", \"JPEG\")\n",
        "\n",
        "    out = cnn(img_arr.unsqueeze(0))\n",
        "    out_img = ToPILImage()(out.squeeze(0))\n",
        "    out_img.save(\"out.jpg\", \"JPEG\")\n",
        "    \n",
        "  index += 1\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s3F_G5alDm_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "\n",
        "With regular upscale (using resize() function), the psnr is about 28.\n",
        "\n",
        "Under setting batch_size = 64, epochs = 100:\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KW7fuFddDm_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"model-unet.pkl\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"scale\": 3,\n",
        "    \"resize\": 480,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 42,\n",
        "    \"optimizor\": \"Adam\",\n",
        "    \"train_set\": \"ImageNet\"\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = UNet(3, 32, 1, 1)\n",
        "cnn = train(args, cnn)"
      ],
      "metadata": {
        "id": "BXXCMyrWQMZ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}