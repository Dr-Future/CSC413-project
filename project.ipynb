{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Future/CSC413-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CURRENT PREGRESS AND NOTES:\n",
        "\n",
        "It takes 10 sec to run 1 epoch on colab\n",
        "\n",
        "After 100 epochs of training the model performs much better. Problems is, in the area where it's supposed to be completely black or white, the model outputs opposite color pixel.\n",
        "\n",
        "Maybe try Adam rather than SGD\n",
        "\n",
        "I forgot to save the work after training for 100 epochs"
      ],
      "metadata": {
        "id": "NSa60u7DEBNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Download"
      ],
      "metadata": {
        "id": "noRLrY17zw2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1YfLyw9jHVIa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import torch\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 67M\n",
        "!curl --remote-name -L https://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\n",
        "!tar xzf BSR_bsds500.tgz\n",
        "!rm BSR_bsds500.tgz\n",
        "!mv BSR/BSDS500/data/images/* BSR/BSDS500\n",
        "!rm -r BSR/bench\n",
        "!rm -r BSR/documentation\n",
        "!rm -r BSR/BSDS500/data\n",
        "\n",
        "# Move all samples in test folder to train folder\n",
        "# So that we have 400 training examples\n",
        "# %mv BSR/BSDS500/data/images/test/*.jpg BSR/BSDS500/data/images/train\n",
        "\n",
        "# 428M\n",
        "!curl --remote-name -L https://image-net.org/data/ILSVRC/2017/ILSVRC2017_DET_test_new.tar.gz\n",
        "!tar xzf ILSVRC2017_DET_test_new.tar.gz\n",
        "!rm ILSVRC2017_DET_test_new.tar.gz\n",
        "!mv ILSVRC/Data/DET/test/*.JPEG ILSVRC\n",
        "!rm -r ILSVRC/Data\n",
        "!rm -r ILSVRC/ImageSets"
      ],
      "metadata": {
        "id": "kjG9_1zyJ5F_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cfcc8f8-12cf-4f14-e0e0-5c8b8fdc32b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 67.4M  100 67.4M    0     0  23.1M      0  0:00:02  0:00:02 --:--:-- 25.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  427M  100  427M    0     0  2881k      0  0:02:32  0:02:32 --:--:-- 2673k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "HWWb-80qDm_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "metadata": {
        "id": "vqQUEaUMzYUA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "class BSDS500Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split, scale, resize):\n",
        "        self.root = \"BSR/BSDS500\"\n",
        "        self.split = split # train / test / val\n",
        "        self.scale = scale\n",
        "        self.resize = resize\n",
        "        self.low_res_size = self.resize // self.scale\n",
        "        self.high_res_size = self.low_res_size * self.scale\n",
        "        self.file_names = [os.path.join(self.root, self.split, file_name) for file_name in os.listdir(os.path.join(self.root, self.split)) if file_name.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Convert to YUV and split the channels\n",
        "        image = Image.open(self.file_names[index]).convert(\"YCbCr\")\n",
        "        y, u, v = image.split()\n",
        "\n",
        "        # Rescale images to square\n",
        "        low_composed = Compose([\n",
        "            Resize([self.low_res_size, self.low_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = Compose([\n",
        "            Resize([self.high_res_size, self.high_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        return { \"y_low\": low_composed(y).cuda(), \"u_low\": low_composed(u).cuda(), \"v_low\": low_composed(v).cuda(),\n",
        "               \"y_high\": high_composed(y).cuda(), \"u_high\": high_composed(u).cuda(), \"v_high\": high_composed(v).cuda(),\n",
        "               \"high\": high_composed(image).cuda() }\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "syvEe3KfDm_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ILSVRC2017Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, scale: int, resize: int):\n",
        "        self.root = \"ILSVRC\"\n",
        "        self.scale = scale\n",
        "        self.resize = resize\n",
        "        self.low_res_size = self.resize // self.scale\n",
        "        self.high_res_size = self.low_res_size * self.scale\n",
        "        self.file_names = [os.path.join(self.root, file_name) for file_name in os.listdir(self.root) if file_name.endswith(\".JPEG\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Convert to YUV and split the channels\n",
        "        image = Image.open(self.file_names[index]).convert(\"YCbCr\")\n",
        "        y, u, v = image.split()\n",
        "\n",
        "        # Rescale images to square\n",
        "        low_composed = Compose([\n",
        "            Resize([self.low_res_size, self.low_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = Compose([\n",
        "            Resize([self.high_res_size, self.high_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        return { \"y_low\": low_composed(y).cuda(), \"u_low\": low_composed(u).cuda(), \"v_low\": low_composed(v).cuda(),\n",
        "               \"y_high\": high_composed(y).cuda(), \"u_high\": high_composed(u).cuda(), \"v_high\": high_composed(v).cuda(),\n",
        "               \"high\": high_composed(image).cuda() }"
      ],
      "metadata": {
        "id": "16MNn2Ms5SSo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate PSNR between two images (tensor)\n",
        "def psnr(img_1, img_2):\n",
        "    mse = torch.mean((img_1 - img_2) ** 2)\n",
        "    if(mse == 0):\n",
        "        return 100\n",
        "    max_pixel = 1\n",
        "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
        "    return psnr"
      ],
      "metadata": {
        "id": "pz-10hTqhkS3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESPCN"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "a0Pf5rbtDm_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.xavier_uniform(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "class ESPCN_model(nn.Module):\n",
        "    def __init__(self, layer1_channel, layer2_channel, upscale_factor):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, layer1_channel, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(layer1_channel, layer2_channel, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(layer2_channel, upscale_factor ** 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(upscale_factor) # This function is literally built for ESPCN\n",
        "        )\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        return out"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vQzDVgEjDm_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet"
      ],
      "metadata": {
        "id": "ODWynRpJP3mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From PA2\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours, num_in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ############### \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(num_in_channels, num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, 2 * num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(2 * num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * num_filters, num_filters, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(num_filters + num_filters, num_colours, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_colours),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.output = nn.Conv2d(num_in_channels + num_colours, num_colours, kernel, padding=padding)\n",
        "        ###################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        a = self.block1(x)\n",
        "        b = self.block2(a)\n",
        "        c = self.block3(b)\n",
        "        c1 = torch.cat((a, c), dim=1)\n",
        "        d = self.block4(c1)\n",
        "        d1 = torch.cat((x, d), dim=1)\n",
        "        return self.output(d1)\n",
        "        ###################################################"
      ],
      "metadata": {
        "id": "0fE-nxSEP5Nw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ttXN0H-QP7aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for validation"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "G9anRABkDm_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "def run_validation_step(\n",
        "    cnn,\n",
        "    criterion,\n",
        "    val_set,\n",
        "):\n",
        "    losses = []\n",
        "    psnrs = []\n",
        "\n",
        "    for i_batch, sample_batched in enumerate(val_set):\n",
        "          input = sample_batched[\"y_low\"]\n",
        "          output = cnn(input)\n",
        "\n",
        "          loss = criterion(output, sample_batched[\"y_high\"])\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "          # Me trying to use psnr in tf\n",
        "          # input_numpy = convert_to_tensor(input.squeeze().cpu().detach().numpy())\n",
        "          # output_numpy = convert_to_tensor(output.squeeze().cpu().detach().numpy())\n",
        "\n",
        "          # regular_upscale = Resize([args.resize, args.resize], InterpolationMode.BICUBIC)(input)\n",
        "\n",
        "          psnrs.append(psnr(output, sample_batched[\"y_high\"]))\n",
        "\n",
        "    val_loss = np.mean(losses)\n",
        "    val_psnr = np.mean(psnrs)\n",
        "    return val_loss, val_psnr"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Zv739LcHDm_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for training (adapted from PA2)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "qtDyptH0Dm_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "source": [
        "def train(args, cnn=None):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # torch.set_num_threads(5)\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.model\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        cnn = ESPCN_model(64, 32, 3)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    if args.optimizor is \"SGD\":\n",
        "      optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4, nesterov=False)\n",
        "    elif args.optimizor is \"Adam\":\n",
        "      optimizer = torch.optim.Adam(cnn.parameters())\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    if args.train_set is \"ImageNet\":\n",
        "      trainset = torch.utils.data.DataLoader(ILSVRC2017Dataset(scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "    elif args.train_set is \"BSDS\":\n",
        "      trainset = torch.utils.data.DataLoader(BSDS500Dataset(split=\"train\", scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "\n",
        "    testset = torch.utils.data.DataLoader(BSDS500Dataset(split=\"val\", scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "\n",
        "    # for item in trainset:\n",
        "    #   print(item)\n",
        "\n",
        "    # y.save(\"temp.jpg\", \"JPEG\")\n",
        "\n",
        "    # print(\"Transforming data...\")\n",
        "    # train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    # train_rgb_cat = get_rgb_cat(train_rgb, colours)\n",
        "    # test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    # test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_PSNR = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "\n",
        "        for i_batch, sample_batched in enumerate(trainset):\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          input = sample_batched[\"y_low\"]\n",
        "          out = cnn(input)\n",
        "\n",
        "          # print(out.size())\n",
        "          # print(sample_batched[\"y_high\"].size())\n",
        "\n",
        "          loss = criterion(out, sample_batched[\"y_high\"])\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "        # plot training images\n",
        "        avg_loss = np.mean(losses)\n",
        "        train_losses.append(avg_loss)\n",
        "        time_elapsed = time.time() - start\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Train Loss: %.4f, Time (s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        # if epoch % 5 == 0:\n",
        "        cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "\n",
        "        val_loss, val_psnr = run_validation_step(cnn, criterion, testset)\n",
        "\n",
        "        time_elapsed = time.time() - start\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_PSNR.append(val_psnr)\n",
        "\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Val Loss: %.4f, Val PSNR: %.4f, Time(s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, val_loss, val_psnr, time_elapsed)\n",
        "          )\n",
        "        # else:\n",
        "        #   valid_losses.append(None)\n",
        "\n",
        "    # Plot training curve\n",
        "    plot1 = plt.figure(1)\n",
        "    plt.plot(train_losses, \"ro-\", label=\"Train\")\n",
        "    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/training_curve.png\")\n",
        "\n",
        "    plot2 = plt.figure(2)\n",
        "    plt.plot(valid_PSNR, \"yo-\", label=\"PSNR\")\n",
        "    plt.legend()\n",
        "    plt.title(\"PSNR\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/PSNR_curve.png\")\n",
        "\n",
        "    if args.checkpoint:\n",
        "        print(\"Saving model...\")\n",
        "        torch.save(cnn.state_dict(), args.checkpoint)\n",
        "\n",
        "    return cnn"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dPEllswFDm_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Oru2s9VqDm_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"model.pkl\",\n",
        "    \"model\": \"ESPCN\",\n",
        "    \"scale\": 3,\n",
        "    \"resize\": 480,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 50,\n",
        "    \"seed\": 42,\n",
        "    \"optimizor\": \"Adam\",\n",
        "    \"train_set\": \"ImageNet\"\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pny-9IhwDm_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BRI8IDUmDm_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "val_set = BSDS500Dataset(split=\"val\", scale=3, resize=480)\n",
        "\n",
        "index = 0\n",
        "\n",
        "# checkpoint = torch.load(\"model_50epochs.pkl\")\n",
        "# cnn.load_state_dict(checkpoint)\n",
        "cnn.eval()\n",
        "\n",
        "for item in val_set:\n",
        "    if index == 50:\n",
        "        img = torch.cat((item[\"y_low\"], item[\"u_low\"], item[\"v_low\"]), 0)\n",
        "        ToPILImage(mode=\"YCbCr\")(img).save(\"in.jpg\", \"JPEG\")\n",
        "\n",
        "        out = torch.cat((\n",
        "            cnn(item[\"y_low\"].unsqueeze(0)),\n",
        "            cnn(item[\"u_low\"].unsqueeze(0)),\n",
        "            cnn(item[\"v_low\"].unsqueeze(0))\n",
        "        ), 0).squeeze(1)\n",
        "        ToPILImage(mode=\"YCbCr\")(out).save(\"out.jpg\", \"JPEG\")\n",
        "\n",
        "    index += 1"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s3F_G5alDm_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "\n",
        "With regular upscale (using resize() function), the psnr is about 28.\n",
        "\n",
        "Under setting batch_size = 64, epochs = 100:\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KW7fuFddDm_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"model-unet.pkl\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"scale\": 3,\n",
        "    \"resize\": 480,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 42,\n",
        "    \"optimizor\": \"Adam\",\n",
        "    \"train_set\": \"ImageNet\"\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = UNet(3, 32, 1, 1)\n",
        "cnn = train(args, cnn)"
      ],
      "metadata": {
        "id": "BXXCMyrWQMZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02625126-86ce-411b-c70c-43e392578ca3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Beginning training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([64, 1, 480, 480])) that is different to the input size (torch.Size([64, 1, 160, 160])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-17baac898ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-6b035cef864a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, cnn)\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0;31m# print(sample_batched[\"y_high\"].size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_high\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (160) must match the size of tensor b (480) at non-singleton dimension 3"
          ]
        }
      ]
    }
  ]
}