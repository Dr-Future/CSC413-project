{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Future/CSC413-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CURRENT PREGRESS AND NOTES:\n",
        "\n",
        "It takes 10 sec to run 1 epoch on colab\n",
        "\n",
        "After 100 epochs of training the model performs much better. Problems is, in the area where it's supposed to be completely black or white, the model outputs opposite color pixel.\n",
        "\n",
        "Maybe try Adam rather than SGD\n",
        "\n",
        "I forgot to save the work after training for 100 epochs"
      ],
      "metadata": {
        "id": "NSa60u7DEBNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --remote-name -sSfL https://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\n",
        "!tar xzf BSR_bsds500.tgz"
      ],
      "metadata": {
        "id": "kjG9_1zyJ5F_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages"
      ],
      "metadata": {
        "id": "qgFvI6b8HdJz",
        "pycharm": {
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1YfLyw9jHVIa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import scipy.misc\n",
        "import torch\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "HWWb-80qDm_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "path = \"BSR/BSDS500/data/images\""
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TF2f6xq1Dm_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "# Move all samples in test folder to train folder\n",
        "# So that we have 400 training examples\n",
        "\n",
        "%mv BSR/BSDS500/data/images/test/*.jpg BSR/BSDS500/data/images/train"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TAeYIIWHDm_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "class BSDS500Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path: str, mode: str, scale: int, resize: int):\n",
        "        self.path = path\n",
        "        self.mode = mode # train / test / val\n",
        "        self.scale = scale\n",
        "        self.resize = resize\n",
        "        self.file_names = [os.path.join(self.path, self.mode, file_name) for file_name in os.listdir(os.path.join(self.path, self.mode)) if file_name.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        file_name = self.file_names[index]\n",
        "        image = Image.open(file_name)\n",
        "\n",
        "        # Convert to YUV and split the channels\n",
        "        image.convert(\"YCbCr\")\n",
        "        y, u, v = image.split()\n",
        "\n",
        "        # Calculate size\n",
        "        low_res_size = self.resize // self.scale\n",
        "        high_res_size = low_res_size * self.scale\n",
        "\n",
        "        # low_res_size = [image.size[1] // self.scale, image.size[0] // self.scale]\n",
        "        # high_res_size = [low_res_size[0] * self.scale, low_res_size[1] * self.scale]\n",
        "\n",
        "        # Rescale images to square\n",
        "        low_composed = Compose([\n",
        "            Resize([low_res_size, low_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = Compose([\n",
        "            Resize([high_res_size, high_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        sample = { \"y_low\": low_composed(y).cuda(), \"u_low\": low_composed(u).cuda(), \"v_low\": low_composed(v).cuda(),\n",
        "               \"y_high\": high_composed(y).cuda(), \"u_high\": high_composed(u).cuda(), \"v_high\": high_composed(v).cuda(),\n",
        "               \"high\": high_composed(image).cuda() }\n",
        "        return sample\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "syvEe3KfDm_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESPCN model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "a0Pf5rbtDm_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "class ESPCN_model(nn.Module):\n",
        "    def __init__(self, layer1_channel, layer2_channel, upscale_factor):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, layer1_channel, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(layer1_channel, layer2_channel, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(layer2_channel, upscale_factor ** 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(upscale_factor) # This function is literally build for ESPCN\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        return out"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vQzDVgEjDm_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From PA2\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours, num_in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ############### \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(num_in_channels, num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, 2 * num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(2 * num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * num_filters, num_filters, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(num_filters + num_filters, num_colours, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_colours),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.output = nn.Conv2d(num_in_channels + num_colours, num_colours, kernel, padding=padding)\n",
        "        ###################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        a = self.block1(x)\n",
        "        b = self.block2(a)\n",
        "        c = self.block3(b)\n",
        "        c1 = torch.cat((a, c), dim=1)\n",
        "        d = self.block4(c1)\n",
        "        d1 = torch.cat((x, d), dim=1)\n",
        "        return self.output(d1)\n",
        "        ###################################################"
      ],
      "metadata": {
        "id": "AiLHONxlob8B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for validation"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "G9anRABkDm_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def run_validation_step(\n",
        "    cnn,\n",
        "    criterion,\n",
        "    val_set,\n",
        "):\n",
        "    losses = []\n",
        "    for i_batch, sample_batched in enumerate(val_set):\n",
        "          input = sample_batched[\"y_low\"]\n",
        "          out = cnn(input)\n",
        "\n",
        "          loss = criterion(out, sample_batched[\"y_high\"])\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "    # if plotpath:  # only plot if a path is provided\n",
        "    #     plot(\n",
        "    #         xs,\n",
        "    #         ys,\n",
        "    #         predicted.cpu().numpy(),\n",
        "    #         colours,\n",
        "    #         plotpath,\n",
        "    #         visualize=visualize,\n",
        "    #         compare_bilinear=downsize_input,\n",
        "    #     )\n",
        "\n",
        "    val_loss = np.mean(losses)\n",
        "    return val_loss"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Zv739LcHDm_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for training (adapted from PA2)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "qtDyptH0Dm_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "from matplotlib import transforms\n",
        "from torch._C import wait\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def train(args, cnn=None):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # torch.set_num_threads(5)\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        cnn = ESPCN_model(64, 32, 3)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4, nesterov=False)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    trainset = torch.utils.data.DataLoader(BSDS500Dataset(path, \"train\", 3, args.resize), batch_size = args.batch_size)\n",
        "    testset = torch.utils.data.DataLoader(BSDS500Dataset(path, \"val\", 3, args.resize), batch_size = args.batch_size)\n",
        "\n",
        "    # for item in trainset:\n",
        "    #   print(item)\n",
        "\n",
        "    # y.save(\"temp.jpg\", \"JPEG\")\n",
        "\n",
        "    # print(\"Transforming data...\")\n",
        "    # train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    # train_rgb_cat = get_rgb_cat(train_rgb, colours)\n",
        "    # test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    # test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "\n",
        "        for i_batch, sample_batched in enumerate(trainset):\n",
        "          input = sample_batched[\"y_low\"]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          out = cnn(input)\n",
        "\n",
        "          # print(out.size())\n",
        "          # print(sample_batched[\"y_high\"].size())\n",
        "\n",
        "          loss = criterion(out, sample_batched[\"y_high\"])\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "        # plot training images\n",
        "        # if args.plot:\n",
        "        #     _, predicted = torch.max(outputs.data, 1, keepdim=True)\n",
        "        #     plot(\n",
        "        #         xs,\n",
        "        #         ys,\n",
        "        #         predicted.cpu().numpy(),\n",
        "        #         colours,\n",
        "        #         save_dir + \"/train_%d.png\" % epoch,\n",
        "        #         args.visualize,\n",
        "        #         args.downsize_input,\n",
        "        #     )\n",
        "\n",
        "        # plot training images\n",
        "        avg_loss = np.mean(losses)\n",
        "        train_losses.append(avg_loss)\n",
        "        time_elapsed = time.time() - start\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Loss: %.4f, Time (s): %d\"\n",
        "            % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        # cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "\n",
        "        # val_loss = run_validation_step(\n",
        "        #     cnn,\n",
        "        #     criterion,\n",
        "        #     testset,\n",
        "        #     args.batch_size,\n",
        "        #     # save_dir + \"/test_%d.png\" % epoch,\n",
        "        # )\n",
        "\n",
        "        # time_elapsed = time.time() - start\n",
        "        # valid_losses.append(val_loss)\n",
        "        # print(\n",
        "        #     \"Epoch [%d/%d], Val Loss: %.4f, Time(s): %.2f\"\n",
        "        #     % (epoch + 1, args.epochs, val_loss, time_elapsed)\n",
        "        # )\n",
        "\n",
        "    # Plot training curve\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, \"ro-\", label=\"Train\")\n",
        "    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/training_curve.png\")\n",
        "\n",
        "    if args.checkpoint:\n",
        "        print(\"Saving model...\")\n",
        "        torch.save(cnn.state_dict(), args.checkpoint)\n",
        "\n",
        "    return cnn"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dPEllswFDm_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Oru2s9VqDm_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Beginning training ...\n",
            "Epoch [1/100], Loss: 0.1278, Time (s): 10\n",
            "Epoch [2/100], Loss: 0.0551, Time (s): 20\n",
            "Epoch [3/100], Loss: 0.0387, Time (s): 29\n",
            "Epoch [4/100], Loss: 0.0270, Time (s): 39\n",
            "Epoch [5/100], Loss: 0.0210, Time (s): 49\n",
            "Epoch [6/100], Loss: 0.0165, Time (s): 60\n",
            "Epoch [7/100], Loss: 0.0139, Time (s): 70\n",
            "Epoch [8/100], Loss: 0.0124, Time (s): 80\n",
            "Epoch [9/100], Loss: 0.0114, Time (s): 90\n",
            "Epoch [10/100], Loss: 0.0109, Time (s): 100\n",
            "Epoch [11/100], Loss: 0.0105, Time (s): 110\n",
            "Epoch [12/100], Loss: 0.0102, Time (s): 120\n",
            "Epoch [13/100], Loss: 0.0100, Time (s): 130\n",
            "Epoch [14/100], Loss: 0.0097, Time (s): 139\n",
            "Epoch [15/100], Loss: 0.0095, Time (s): 149\n",
            "Epoch [16/100], Loss: 0.0093, Time (s): 159\n",
            "Epoch [17/100], Loss: 0.0091, Time (s): 169\n",
            "Epoch [18/100], Loss: 0.0089, Time (s): 179\n",
            "Epoch [19/100], Loss: 0.0088, Time (s): 189\n",
            "Epoch [20/100], Loss: 0.0086, Time (s): 199\n",
            "Epoch [21/100], Loss: 0.0084, Time (s): 209\n",
            "Epoch [22/100], Loss: 0.0083, Time (s): 218\n",
            "Epoch [23/100], Loss: 0.0081, Time (s): 228\n",
            "Epoch [24/100], Loss: 0.0080, Time (s): 238\n",
            "Epoch [25/100], Loss: 0.0078, Time (s): 248\n",
            "Epoch [26/100], Loss: 0.0077, Time (s): 258\n",
            "Epoch [27/100], Loss: 0.0076, Time (s): 268\n",
            "Epoch [28/100], Loss: 0.0075, Time (s): 278\n",
            "Epoch [29/100], Loss: 0.0073, Time (s): 288\n",
            "Epoch [30/100], Loss: 0.0072, Time (s): 298\n",
            "Epoch [31/100], Loss: 0.0071, Time (s): 307\n",
            "Epoch [32/100], Loss: 0.0070, Time (s): 317\n",
            "Epoch [33/100], Loss: 0.0069, Time (s): 327\n",
            "Epoch [34/100], Loss: 0.0068, Time (s): 337\n",
            "Epoch [35/100], Loss: 0.0067, Time (s): 347\n",
            "Epoch [36/100], Loss: 0.0066, Time (s): 357\n",
            "Epoch [37/100], Loss: 0.0065, Time (s): 367\n",
            "Epoch [38/100], Loss: 0.0064, Time (s): 376\n",
            "Epoch [39/100], Loss: 0.0063, Time (s): 386\n",
            "Epoch [40/100], Loss: 0.0062, Time (s): 396\n",
            "Epoch [41/100], Loss: 0.0061, Time (s): 406\n",
            "Epoch [42/100], Loss: 0.0061, Time (s): 416\n",
            "Epoch [43/100], Loss: 0.0060, Time (s): 426\n",
            "Epoch [44/100], Loss: 0.0059, Time (s): 436\n",
            "Epoch [45/100], Loss: 0.0058, Time (s): 446\n",
            "Epoch [46/100], Loss: 0.0058, Time (s): 455\n",
            "Epoch [47/100], Loss: 0.0057, Time (s): 465\n",
            "Epoch [48/100], Loss: 0.0056, Time (s): 475\n",
            "Epoch [49/100], Loss: 0.0056, Time (s): 485\n",
            "Epoch [50/100], Loss: 0.0055, Time (s): 495\n",
            "Epoch [51/100], Loss: 0.0055, Time (s): 505\n",
            "Epoch [52/100], Loss: 0.0054, Time (s): 515\n",
            "Epoch [53/100], Loss: 0.0054, Time (s): 525\n",
            "Epoch [54/100], Loss: 0.0053, Time (s): 535\n",
            "Epoch [55/100], Loss: 0.0053, Time (s): 545\n",
            "Epoch [56/100], Loss: 0.0052, Time (s): 554\n",
            "Epoch [57/100], Loss: 0.0052, Time (s): 564\n",
            "Epoch [58/100], Loss: 0.0051, Time (s): 574\n",
            "Epoch [59/100], Loss: 0.0051, Time (s): 584\n",
            "Epoch [60/100], Loss: 0.0050, Time (s): 594\n",
            "Epoch [61/100], Loss: 0.0050, Time (s): 604\n",
            "Epoch [62/100], Loss: 0.0049, Time (s): 614\n",
            "Epoch [63/100], Loss: 0.0049, Time (s): 624\n",
            "Epoch [64/100], Loss: 0.0049, Time (s): 634\n",
            "Epoch [65/100], Loss: 0.0048, Time (s): 644\n",
            "Epoch [66/100], Loss: 0.0048, Time (s): 654\n",
            "Epoch [67/100], Loss: 0.0047, Time (s): 663\n",
            "Epoch [68/100], Loss: 0.0047, Time (s): 673\n",
            "Epoch [69/100], Loss: 0.0047, Time (s): 683\n",
            "Epoch [70/100], Loss: 0.0046, Time (s): 693\n",
            "Epoch [71/100], Loss: 0.0046, Time (s): 703\n",
            "Epoch [72/100], Loss: 0.0046, Time (s): 713\n",
            "Epoch [73/100], Loss: 0.0045, Time (s): 723\n",
            "Epoch [74/100], Loss: 0.0045, Time (s): 733\n",
            "Epoch [75/100], Loss: 0.0045, Time (s): 743\n",
            "Epoch [76/100], Loss: 0.0045, Time (s): 752\n",
            "Epoch [77/100], Loss: 0.0044, Time (s): 762\n",
            "Epoch [78/100], Loss: 0.0044, Time (s): 772\n",
            "Epoch [79/100], Loss: 0.0044, Time (s): 782\n",
            "Epoch [80/100], Loss: 0.0043, Time (s): 792\n",
            "Epoch [81/100], Loss: 0.0043, Time (s): 802\n",
            "Epoch [82/100], Loss: 0.0043, Time (s): 812\n",
            "Epoch [83/100], Loss: 0.0043, Time (s): 822\n",
            "Epoch [84/100], Loss: 0.0042, Time (s): 832\n",
            "Epoch [85/100], Loss: 0.0042, Time (s): 841\n",
            "Epoch [86/100], Loss: 0.0042, Time (s): 851\n",
            "Epoch [87/100], Loss: 0.0042, Time (s): 861\n",
            "Epoch [88/100], Loss: 0.0041, Time (s): 871\n",
            "Epoch [89/100], Loss: 0.0041, Time (s): 881\n",
            "Epoch [90/100], Loss: 0.0041, Time (s): 891\n",
            "Epoch [91/100], Loss: 0.0041, Time (s): 901\n",
            "Epoch [92/100], Loss: 0.0041, Time (s): 910\n",
            "Epoch [93/100], Loss: 0.0040, Time (s): 920\n",
            "Epoch [94/100], Loss: 0.0040, Time (s): 930\n",
            "Epoch [95/100], Loss: 0.0040, Time (s): 940\n",
            "Epoch [96/100], Loss: 0.0040, Time (s): 950\n",
            "Epoch [97/100], Loss: 0.0040, Time (s): 960\n",
            "Epoch [98/100], Loss: 0.0039, Time (s): 970\n",
            "Epoch [99/100], Loss: 0.0039, Time (s): 980\n",
            "Epoch [100/100], Loss: 0.0039, Time (s): 989\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"model.pkl\",\n",
        "    \"model\": \"ESPCN\",\n",
        "    \"resize\": 481,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 42,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"ESPCN\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pny-9IhwDm_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "459b7c76-cc46-4a08-c997-99efc4a1dcd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BRI8IDUmDm_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "val_set = BSDS500Dataset(path, \"val\", 3, 481)\n",
        "\n",
        "index = 0\n",
        "cnn.eval()\n",
        "for item in val_set:\n",
        "\n",
        "  if index == 50:\n",
        "    img_arr = item[\"y_low\"]\n",
        "    img = ToPILImage()(img_arr)\n",
        "    img.save(\"in.jpg\", \"JPEG\")\n",
        "\n",
        "    out = cnn(img_arr.unsqueeze(0))\n",
        "    out_img = ToPILImage()(out.squeeze(0))\n",
        "    out_img.save(\"out.jpg\", \"JPEG\")\n",
        "\n",
        "  index += 1\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s3F_G5alDm_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "Under setting batch_size = 64, epochs = 100:\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KW7fuFddDm_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"model-unet.pkl\",\n",
        "    \"model\": \"unused parameter\",\n",
        "    \"resize\": 480,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 42,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"UNet\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = UNet(3, 32, 1, 1)\n",
        "cnn = train(args, cnn)"
      ],
      "metadata": {
        "id": "ewdOLNAy9rcG",
        "outputId": "1f44f948-979f-4ece-e54d-a7b919b87f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Beginning training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([64, 1, 480, 480])) that is different to the input size (torch.Size([64, 1, 160, 160])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9fe409b80f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-bbff5a0a3762>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, cnn)\u001b[0m\n\u001b[1;32m     71\u001b[0m           \u001b[0;31m# print(sample_batched[\"y_high\"].size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_high\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (160) must match the size of tensor b (480) at non-singleton dimension 3"
          ]
        }
      ]
    }
  ]
}