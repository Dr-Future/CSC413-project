{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ecc843b6b3e48d1b7cf2c27a9fc262c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68446c33b8624fdbb643a19a21f43f39",
              "IPY_MODEL_dbc4c0a38b2e40668cb10bf76736a2d8",
              "IPY_MODEL_ddaaf5a08f7049bebf483d22e26dbf46"
            ],
            "layout": "IPY_MODEL_b573a1cfe31040ada4ff2d6dbe3566ab"
          }
        },
        "68446c33b8624fdbb643a19a21f43f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e133e99b99f4aed857e175ed09175a4",
            "placeholder": "​",
            "style": "IPY_MODEL_98de1f85b19f47e9a658f3dc8e8bff9a",
            "value": "100%"
          }
        },
        "dbc4c0a38b2e40668cb10bf76736a2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7a74d866a1482bb9a1da8d66d7a1ab",
            "max": 5500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00087490453b412f8614d36d808c293d",
            "value": 5500
          }
        },
        "ddaaf5a08f7049bebf483d22e26dbf46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40d6dec30084f36bfaad56b41ee1cd1",
            "placeholder": "​",
            "style": "IPY_MODEL_2744df27025341aebd897587c29d3837",
            "value": " 5500/5500 [01:17&lt;00:00, 71.17it/s]"
          }
        },
        "b573a1cfe31040ada4ff2d6dbe3566ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e133e99b99f4aed857e175ed09175a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98de1f85b19f47e9a658f3dc8e8bff9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca7a74d866a1482bb9a1da8d66d7a1ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00087490453b412f8614d36d808c293d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a40d6dec30084f36bfaad56b41ee1cd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2744df27025341aebd897587c29d3837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dr-Future/CSC413-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CURRENT PREGRESS AND NOTES:\n",
        "\n",
        "After 100 epochs of training the model performs much better. Problems is, in the area where it's supposed to be completely black or white, the model outputs opposite color pixel.\n",
        "\n",
        "Maybe try Adam rather than SGD\n",
        "\n",
        "I forgot to save the work after training for 100 epochs"
      ],
      "metadata": {
        "id": "NSa60u7DEBNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Download"
      ],
      "metadata": {
        "id": "noRLrY17zw2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1YfLyw9jHVIa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import torch\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 67M\n",
        "!curl --remote-name -L https://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\n",
        "!tar xzf BSR_bsds500.tgz\n",
        "!rm BSR_bsds500.tgz\n",
        "!mv BSR/BSDS500/data/images/* BSR/BSDS500\n",
        "!rm -r BSR/bench\n",
        "!rm -r BSR/documentation\n",
        "!rm -r BSR/BSDS500/data\n",
        "\n",
        "# Move all samples in test folder to train folder\n",
        "# So that we have 400 training examples\n",
        "# %mv BSR/BSDS500/data/images/test/*.jpg BSR/BSDS500/data/images/train\n",
        "\n",
        "# 428M\n",
        "!curl --remote-name -L https://image-net.org/data/ILSVRC/2017/ILSVRC2017_DET_test_new.tar.gz\n",
        "!tar xzf ILSVRC2017_DET_test_new.tar.gz\n",
        "!rm ILSVRC2017_DET_test_new.tar.gz\n",
        "!mv ILSVRC/Data/DET/test/*.JPEG ILSVRC\n",
        "!rm -r ILSVRC/Data\n",
        "!rm -r ILSVRC/ImageSets"
      ],
      "metadata": {
        "id": "kjG9_1zyJ5F_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f766a02-1f63-4f06-9bb1-7ace7d9a3f40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 67.4M  100 67.4M    0     0  33.5M      0  0:00:02  0:00:02 --:--:-- 40.2M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  427M  100  427M    0     0  13.9M      0  0:00:30  0:00:30 --:--:-- 13.6M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "HWWb-80qDm_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self"
      ],
      "metadata": {
        "id": "vqQUEaUMzYUA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "class BSDS500Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split, scale, resize):\n",
        "        self.root = \"BSR/BSDS500\"\n",
        "        self.split = split # train / test / val\n",
        "        self.scale = scale\n",
        "        self.resize = resize\n",
        "        self.low_res_size = self.resize // self.scale\n",
        "        self.high_res_size = self.low_res_size * self.scale\n",
        "        self.file_names = [os.path.join(self.root, self.split, file_name) for file_name in os.listdir(os.path.join(self.root, self.split)) if file_name.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Convert to YUV and split the channels\n",
        "        image = Image.open(self.file_names[index]).convert(\"YCbCr\")\n",
        "        y, u, v = image.split()\n",
        "\n",
        "        # Rescale images to square\n",
        "        low_composed = Compose([\n",
        "            Resize([self.low_res_size, self.low_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = Compose([\n",
        "            Resize([self.high_res_size, self.high_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        return { \"y_low\": low_composed(y).cuda(), \"u_low\": low_composed(u).cuda(), \"v_low\": low_composed(v).cuda(),\n",
        "               \"y_high\": high_composed(y).cuda() }\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "syvEe3KfDm_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ILSVRC2017Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, scale: int, resize: int):\n",
        "        self.root = \"ILSVRC\"\n",
        "        self.scale = scale\n",
        "        self.resize = resize\n",
        "        self.low_res_size = self.resize // self.scale\n",
        "        self.high_res_size = self.low_res_size * self.scale\n",
        "        self.file_names = [os.path.join(self.root, file_name) for file_name in os.listdir(self.root) if file_name.endswith(\".JPEG\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Convert to YUV and split the channels\n",
        "        image = Image.open(self.file_names[index]).convert(\"YCbCr\")\n",
        "        y, u, v = image.split()\n",
        "\n",
        "        # Rescale images to square\n",
        "        low_composed = Compose([\n",
        "            Resize([self.low_res_size, self.low_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = Compose([\n",
        "            Resize([self.high_res_size, self.high_res_size], interpolation=InterpolationMode.BICUBIC),\n",
        "            ToTensor()\n",
        "        ])\n",
        "\n",
        "        return { \"y_low\": low_composed(y).cuda(), \"y_high\": high_composed(y).cuda() }"
      ],
      "metadata": {
        "id": "16MNn2Ms5SSo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate PSNR between two images (tensor)\n",
        "def psnr(img_1, img_2):\n",
        "    mse = torch.mean((img_1 - img_2) ** 2)\n",
        "    if(mse == 0):\n",
        "        return 100\n",
        "    max_pixel = 1\n",
        "    psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
        "    return psnr"
      ],
      "metadata": {
        "id": "pz-10hTqhkS3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ESPCN"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "a0Pf5rbtDm_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "class ESPCN_model(nn.Module):\n",
        "    def __init__(self, layer1_channel, layer2_channel, upscale_factor):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, layer1_channel, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(layer1_channel, layer2_channel, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(layer2_channel, upscale_factor ** 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(upscale_factor) # This function is literally built for ESPCN\n",
        "        )\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        return out"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vQzDVgEjDm_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet"
      ],
      "metadata": {
        "id": "ODWynRpJP3mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From PA2\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, kernel, num_filters, num_colours, num_in_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Useful parameters\n",
        "        stride = 2\n",
        "        padding = kernel // 2\n",
        "        output_padding = 1\n",
        "\n",
        "        ############### YOUR CODE GOES HERE ############### \n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(num_in_channels, num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, 2 * num_filters, kernel, stride, padding),\n",
        "            nn.BatchNorm2d(2 * num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2 * num_filters, num_filters, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(num_filters + num_filters, num_colours, kernel, stride, padding, output_padding, dilation=1),\n",
        "            nn.BatchNorm2d(num_colours),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.output = nn.Conv2d(num_in_channels + num_colours, num_colours, kernel, padding=padding)\n",
        "        ###################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############### YOUR CODE GOES HERE ###############\n",
        "        a = self.block1(x)\n",
        "        b = self.block2(a)\n",
        "        c = self.block3(b)\n",
        "        c1 = torch.cat((a, c), dim=1)\n",
        "        d = self.block4(c1)\n",
        "        d1 = torch.cat((x, d), dim=1)\n",
        "        return self.output(d1)\n",
        "        ###################################################"
      ],
      "metadata": {
        "id": "0fE-nxSEP5Nw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ttXN0H-QP7aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for validation"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "G9anRABkDm_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "def run_validation_step(\n",
        "    cnn,\n",
        "    criterion,\n",
        "    val_set,\n",
        "):\n",
        "    losses = []\n",
        "    psnrs = []\n",
        "\n",
        "    for i_batch, sample_batched in enumerate(val_set):\n",
        "          input = sample_batched[\"y_low\"]\n",
        "          output = cnn(input)\n",
        "\n",
        "          loss = criterion(output, sample_batched[\"y_high\"])\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "          # Me trying to use psnr in tensorflow\n",
        "          # input_numpy = convert_to_tensor(input.squeeze().cpu().detach().numpy())\n",
        "          # output_numpy = convert_to_tensor(output.squeeze().cpu().detach().numpy())\n",
        "\n",
        "          # regular_upscale = Resize([args.resize, args.resize], InterpolationMode.BICUBIC)(input)\n",
        "\n",
        "          psnrs.append(psnr(output.squeeze(), sample_batched[\"y_high\"].squeeze()))\n",
        "\n",
        "    val_loss = np.mean(losses)\n",
        "    val_psnr = np.mean(psnrs)\n",
        "    return val_loss, val_psnr"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Zv739LcHDm_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_psnr():\n",
        "  "
      ],
      "metadata": {
        "id": "FqNburpUqahj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for training (adapted from PA2)"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "qtDyptH0Dm_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "def train(args, cnn=None):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # torch.set_num_threads(5)\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.model\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        cnn = ESPCN_model(64, 32, 3)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    if args.optimizor is \"SGD\":\n",
        "      optimizer = torch.optim.SGD(cnn.parameters(), lr=args.learn_rate, momentum=0.9, weight_decay=1e-4, nesterov=False)\n",
        "    elif args.optimizor is \"Adam\":\n",
        "      optimizer = torch.optim.Adam(cnn.parameters(), lr=args.learn_rate)\n",
        "\n",
        "    # scheduler = ReduceLROnPlateau(optimizer, patience=5, min_lr=0.0001, verbose=True)\n",
        "    scheduler = ExponentialLR(optimizer, gamma=0.9, verbose=True)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    if args.train_set is \"ImageNet\":\n",
        "      trainset = torch.utils.data.DataLoader(ILSVRC2017Dataset(scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "    elif args.train_set is \"BSDS\":\n",
        "      trainset = torch.utils.data.DataLoader(BSDS500Dataset(split=\"train\", scale=args.scale, resize=args.resize), batch_size=args.batch_size)\n",
        "\n",
        "    testset = torch.utils.data.DataLoader(BSDS500Dataset(split=\"val\", scale=args.scale, resize=args.resize), batch_size=1)\n",
        "\n",
        "    # for item in trainset:\n",
        "    #   print(item)\n",
        "\n",
        "    # y.save(\"temp.jpg\", \"JPEG\")\n",
        "\n",
        "    # print(\"Transforming data...\")\n",
        "    # train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    # train_rgb_cat = get_rgb_cat(train_rgb, colours)\n",
        "    # test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    # test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_PSNR = []\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "\n",
        "        for i_batch, sample_batched in enumerate(trainset):\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          input = sample_batched[\"y_low\"]\n",
        "          out = cnn(input)\n",
        "\n",
        "          # print(out.size())\n",
        "          # print(sample_batched[\"y_high\"].size())\n",
        "\n",
        "          loss = criterion(out, sample_batched[\"y_high\"])\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          losses.append(loss.data.item())\n",
        "\n",
        "        # plot training images\n",
        "        avg_loss = np.mean(losses)\n",
        "        train_losses.append(avg_loss)\n",
        "        time_elapsed = time.time() - start\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Train Loss: %.4f, Time (s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        cnn.eval()  # Change model to 'eval' mode\n",
        "\n",
        "        val_loss, val_psnr = run_validation_step(cnn, criterion, testset)\n",
        "\n",
        "        time_elapsed = time.time() - start\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_PSNR.append(val_psnr)\n",
        "\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Val Loss: %.4f, Val PSNR: %.4f, Time(s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, val_loss, val_psnr, time_elapsed)\n",
        "          )\n",
        "        \n",
        "        # Learning rate decay\n",
        "        curr_lr = optimizer.param_groups[0]['lr']\n",
        "        if epoch > 10 and curr_lr > 0.0001:\n",
        "          scheduler.step()\n",
        "\n",
        "        if curr_lr < 0.0001:\n",
        "          for g in optimizer.param_groups:\n",
        "            g['lr'] = 0.0001\n",
        "\n",
        "    # Plot training curve\n",
        "    plot1 = plt.figure(1)\n",
        "    plt.plot(train_losses, \"ro-\", label=\"Train\")\n",
        "    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/training_curve.png\")\n",
        "\n",
        "    plot2 = plt.figure(2)\n",
        "    plt.plot(valid_PSNR, \"yo-\", label=\"PSNR\")\n",
        "    plt.legend()\n",
        "    plt.title(\"PSNR\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/PSNR_curve.png\")\n",
        "\n",
        "    print(\"Final learning rate: %.4f\" %optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    if args.checkpoint:\n",
        "        print(\"Saving model...\")\n",
        "        torch.save(cnn.state_dict(), args.checkpoint)\n",
        "\n",
        "    return cnn, train_losses, valid_losses, valid_PSNR"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dPEllswFDm_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Oru2s9VqDm_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Loading data...\n",
            "Beginning training ...\n",
            "Epoch [1/50], Train Loss: 0.0150, Time (s): 75.89\n",
            "Epoch [1/50], Val Loss: 0.0047, Val PSNR: 23.9852, Time(s): 77.55\n",
            "Epoch [2/50], Train Loss: 0.0049, Time (s): 150.30\n",
            "Epoch [2/50], Val Loss: 0.0030, Val PSNR: 26.0462, Time(s): 151.74\n",
            "Epoch [3/50], Train Loss: 0.0038, Time (s): 225.58\n",
            "Epoch [3/50], Val Loss: 0.0025, Val PSNR: 27.0129, Time(s): 227.62\n",
            "Epoch [4/50], Train Loss: 0.0034, Time (s): 300.56\n",
            "Epoch [4/50], Val Loss: 0.0023, Val PSNR: 27.4104, Time(s): 301.93\n",
            "Epoch [5/50], Train Loss: 0.0032, Time (s): 376.30\n",
            "Epoch [5/50], Val Loss: 0.0022, Val PSNR: 27.5177, Time(s): 377.71\n",
            "Epoch [6/50], Train Loss: 0.0031, Time (s): 451.28\n",
            "Epoch [6/50], Val Loss: 0.0021, Val PSNR: 27.9826, Time(s): 452.72\n",
            "Epoch [7/50], Train Loss: 0.0030, Time (s): 526.15\n",
            "Epoch [7/50], Val Loss: 0.0020, Val PSNR: 28.0394, Time(s): 527.58\n",
            "Epoch [8/50], Train Loss: 0.0029, Time (s): 600.48\n",
            "Epoch [8/50], Val Loss: 0.0020, Val PSNR: 28.0922, Time(s): 601.86\n",
            "Epoch [9/50], Train Loss: 0.0029, Time (s): 673.88\n",
            "Epoch [9/50], Val Loss: 0.0019, Val PSNR: 28.2851, Time(s): 675.26\n",
            "Epoch [10/50], Train Loss: 0.0029, Time (s): 748.61\n",
            "Epoch [10/50], Val Loss: 0.0019, Val PSNR: 28.3789, Time(s): 750.03\n",
            "Epoch [11/50], Train Loss: 0.0028, Time (s): 823.88\n",
            "Epoch [11/50], Val Loss: 0.0019, Val PSNR: 28.4426, Time(s): 825.27\n",
            "Epoch [12/50], Train Loss: 0.0028, Time (s): 901.08\n",
            "Epoch [12/50], Val Loss: 0.0019, Val PSNR: 28.3553, Time(s): 902.57\n",
            "Adjusting learning rate of group 0 to 9.0000e-04.\n",
            "Epoch [13/50], Train Loss: 0.0028, Time (s): 978.01\n",
            "Epoch [13/50], Val Loss: 0.0019, Val PSNR: 28.4831, Time(s): 979.44\n",
            "Adjusting learning rate of group 0 to 8.1000e-04.\n",
            "Epoch [14/50], Train Loss: 0.0028, Time (s): 1052.57\n",
            "Epoch [14/50], Val Loss: 0.0019, Val PSNR: 28.4296, Time(s): 1053.91\n",
            "Adjusting learning rate of group 0 to 7.2900e-04.\n",
            "Epoch [15/50], Train Loss: 0.0028, Time (s): 1125.23\n",
            "Epoch [15/50], Val Loss: 0.0019, Val PSNR: 28.4755, Time(s): 1126.58\n",
            "Adjusting learning rate of group 0 to 6.5610e-04.\n",
            "Epoch [16/50], Train Loss: 0.0028, Time (s): 1198.05\n",
            "Epoch [16/50], Val Loss: 0.0019, Val PSNR: 28.5309, Time(s): 1199.44\n",
            "Adjusting learning rate of group 0 to 5.9049e-04.\n",
            "Epoch [17/50], Train Loss: 0.0028, Time (s): 1270.59\n",
            "Epoch [17/50], Val Loss: 0.0019, Val PSNR: 28.5442, Time(s): 1271.99\n",
            "Adjusting learning rate of group 0 to 5.3144e-04.\n",
            "Epoch [18/50], Train Loss: 0.0028, Time (s): 1342.71\n",
            "Epoch [18/50], Val Loss: 0.0019, Val PSNR: 28.5496, Time(s): 1344.07\n",
            "Adjusting learning rate of group 0 to 4.7830e-04.\n",
            "Epoch [19/50], Train Loss: 0.0028, Time (s): 1415.52\n",
            "Epoch [19/50], Val Loss: 0.0019, Val PSNR: 28.5557, Time(s): 1416.94\n",
            "Adjusting learning rate of group 0 to 4.3047e-04.\n",
            "Epoch [20/50], Train Loss: 0.0028, Time (s): 1490.06\n",
            "Epoch [20/50], Val Loss: 0.0018, Val PSNR: 28.5608, Time(s): 1491.44\n",
            "Adjusting learning rate of group 0 to 3.8742e-04.\n",
            "Epoch [21/50], Train Loss: 0.0028, Time (s): 1564.32\n",
            "Epoch [21/50], Val Loss: 0.0018, Val PSNR: 28.5652, Time(s): 1565.69\n",
            "Adjusting learning rate of group 0 to 3.4868e-04.\n",
            "Epoch [22/50], Train Loss: 0.0027, Time (s): 1638.89\n",
            "Epoch [22/50], Val Loss: 0.0018, Val PSNR: 28.5694, Time(s): 1640.31\n",
            "Adjusting learning rate of group 0 to 3.1381e-04.\n",
            "Epoch [23/50], Train Loss: 0.0027, Time (s): 1713.11\n",
            "Epoch [23/50], Val Loss: 0.0018, Val PSNR: 28.5731, Time(s): 1714.49\n",
            "Adjusting learning rate of group 0 to 2.8243e-04.\n",
            "Epoch [24/50], Train Loss: 0.0027, Time (s): 1786.00\n",
            "Epoch [24/50], Val Loss: 0.0018, Val PSNR: 28.5742, Time(s): 1787.36\n",
            "Adjusting learning rate of group 0 to 2.5419e-04.\n",
            "Epoch [25/50], Train Loss: 0.0027, Time (s): 1862.39\n",
            "Epoch [25/50], Val Loss: 0.0018, Val PSNR: 28.5779, Time(s): 1863.86\n",
            "Adjusting learning rate of group 0 to 2.2877e-04.\n",
            "Epoch [26/50], Train Loss: 0.0027, Time (s): 1937.13\n",
            "Epoch [26/50], Val Loss: 0.0018, Val PSNR: 28.5792, Time(s): 1938.56\n",
            "Adjusting learning rate of group 0 to 2.0589e-04.\n",
            "Epoch [27/50], Train Loss: 0.0027, Time (s): 2011.07\n",
            "Epoch [27/50], Val Loss: 0.0018, Val PSNR: 28.5768, Time(s): 2012.41\n",
            "Adjusting learning rate of group 0 to 1.8530e-04.\n",
            "Epoch [28/50], Train Loss: 0.0027, Time (s): 2083.63\n",
            "Epoch [28/50], Val Loss: 0.0018, Val PSNR: 28.5802, Time(s): 2084.96\n",
            "Adjusting learning rate of group 0 to 1.6677e-04.\n",
            "Epoch [29/50], Train Loss: 0.0027, Time (s): 2155.85\n",
            "Epoch [29/50], Val Loss: 0.0018, Val PSNR: 28.5854, Time(s): 2157.19\n",
            "Adjusting learning rate of group 0 to 1.5009e-04.\n",
            "Epoch [30/50], Train Loss: 0.0027, Time (s): 2229.15\n",
            "Epoch [30/50], Val Loss: 0.0018, Val PSNR: 28.5871, Time(s): 2230.53\n",
            "Adjusting learning rate of group 0 to 1.3509e-04.\n",
            "Epoch [31/50], Train Loss: 0.0027, Time (s): 2302.24\n",
            "Epoch [31/50], Val Loss: 0.0018, Val PSNR: 28.5894, Time(s): 2303.58\n",
            "Adjusting learning rate of group 0 to 1.2158e-04.\n",
            "Epoch [32/50], Train Loss: 0.0027, Time (s): 2374.60\n",
            "Epoch [32/50], Val Loss: 0.0018, Val PSNR: 28.5924, Time(s): 2375.94\n",
            "Adjusting learning rate of group 0 to 1.0942e-04.\n",
            "Epoch [33/50], Train Loss: 0.0027, Time (s): 2446.79\n",
            "Epoch [33/50], Val Loss: 0.0018, Val PSNR: 28.5939, Time(s): 2448.13\n",
            "Adjusting learning rate of group 0 to 9.8477e-05.\n",
            "Epoch [34/50], Train Loss: 0.0027, Time (s): 2519.55\n",
            "Epoch [34/50], Val Loss: 0.0018, Val PSNR: 28.5947, Time(s): 2520.91\n",
            "Epoch [35/50], Train Loss: 0.0027, Time (s): 2591.36\n",
            "Epoch [35/50], Val Loss: 0.0018, Val PSNR: 28.5963, Time(s): 2592.70\n",
            "Epoch [36/50], Train Loss: 0.0027, Time (s): 2662.96\n",
            "Epoch [36/50], Val Loss: 0.0018, Val PSNR: 28.5974, Time(s): 2664.31\n"
          ]
        }
      ],
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": True,\n",
        "    \"checkpoint\": \"model.pkl\",\n",
        "    \"model\": \"ESPCN\",\n",
        "    \"scale\": 3,\n",
        "    \"resize\": 480,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 50,\n",
        "    \"seed\": 42,\n",
        "    \"optimizor\": \"Adam\",\n",
        "    \"train_set\": \"ImageNet\"\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn, train_loss, val_loss, val_PSNR = train(args)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pny-9IhwDm_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc67cd2-bb01-4b50-b7ec-f796ee3e7177"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BRI8IDUmDm_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ecc843b6b3e48d1b7cf2c27a9fc262c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28.066229113340473\n",
            "26.890662967055878\n"
          ]
        }
      ],
      "source": [
        "# val_set = BSDS500Dataset(split=\"val\", scale=3, resize=480)\n",
        "val_set = ILSVRC2017Dataset(scale=args.scale, resize=args.resize)\n",
        "\n",
        "index = 0\n",
        "cnn.eval()\n",
        "model_psnr = []\n",
        "bicubic_psnr = []\n",
        "for item in tqdm(val_set):\n",
        "\n",
        "  # Compute average PSNR on given val_set\n",
        "  img_arr = item[\"y_low\"]\n",
        "  out = cnn(img_arr.unsqueeze(0))\n",
        "  regular_upscale = Resize([args.resize, args.resize], InterpolationMode.BICUBIC)(img_arr)\n",
        "  bicubic_psnr.append(psnr(regular_upscale, item[\"y_high\"]))\n",
        "  model_psnr.append(psnr(out, item[\"y_high\"]))\n",
        "\n",
        "  # Visualization\n",
        "  # if index == -1:\n",
        "  #   img_arr = item[\"y_low\"]\n",
        "  #   img = ToPILImage()(img_arr)\n",
        "  #   img.save(\"in.jpg\", \"JPEG\")\n",
        "\n",
        "  #   out = cnn(img_arr.unsqueeze(0))\n",
        "  #   out_img = ToPILImage()(out.squeeze(0))\n",
        "  #   out_img.save(\"out.jpg\", \"JPEG\")\n",
        "  #   regular_upscale = Resize([args.resize, args.resize], InterpolationMode.BICUBIC)(img_arr)\n",
        "    \n",
        "  index += 1\n",
        "\n",
        "print(np.mean(model_psnr))\n",
        "print(np.mean(bicubic_psnr))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "s3F_G5alDm_T",
        "outputId": "0d3cd503-c549-4b1e-fb97-d7a203ebb0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "2ecc843b6b3e48d1b7cf2c27a9fc262c",
            "68446c33b8624fdbb643a19a21f43f39",
            "dbc4c0a38b2e40668cb10bf76736a2d8",
            "ddaaf5a08f7049bebf483d22e26dbf46",
            "b573a1cfe31040ada4ff2d6dbe3566ab",
            "3e133e99b99f4aed857e175ed09175a4",
            "98de1f85b19f47e9a658f3dc8e8bff9a",
            "ca7a74d866a1482bb9a1da8d66d7a1ab",
            "00087490453b412f8614d36d808c293d",
            "a40d6dec30084f36bfaad56b41ee1cd1",
            "2744df27025341aebd897587c29d3837"
          ]
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:\n",
        "\n",
        "\n",
        "With regular upscale (using resize() function), the psnr is about 28.\n",
        "\n",
        "Under setting optimizer = Adam, batch_size = 64, the loss stablizes after 25 epochs with following data:\n",
        "\n",
        "Epoch [25/50], Train Loss: 0.0028, Time (s): 3430.96\n",
        "\n",
        "Epoch [25/50], Val Loss: 0.0018, Val PSNR: 27.4064, Time(s): 3433.18\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KW7fuFddDm_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"model-unet.pkl\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"scale\": 3,\n",
        "    \"resize\": 480,  # Images will be transformed to [resize, resize] square image\n",
        "    \"kernel\": 3,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"epochs\": 100,\n",
        "    \"seed\": 42,\n",
        "    \"optimizor\": \"Adam\",\n",
        "    \"train_set\": \"ImageNet\"\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = UNet(3, 32, 1, 1)\n",
        "cnn = train(args, cnn)"
      ],
      "metadata": {
        "id": "BXXCMyrWQMZ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02625126-86ce-411b-c70c-43e392578ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Beginning training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([64, 1, 480, 480])) that is different to the input size (torch.Size([64, 1, 160, 160])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-17baac898ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-6b035cef864a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, cnn)\u001b[0m\n\u001b[1;32m     72\u001b[0m           \u001b[0;31m# print(sample_batched[\"y_high\"].size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_high\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (160) must match the size of tensor b (480) at non-singleton dimension 3"
          ]
        }
      ]
    }
  ]
}