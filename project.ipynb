{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import packages"
      ],
      "metadata": {
        "id": "qgFvI6b8HdJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1YfLyw9jHVIa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "from PIL import Image\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import scipy.misc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "origin = \"https://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\"\n",
        "f = \"BSR_bsds500.tar.gz\"\n",
        "\n",
        "try:\n",
        "    try:\n",
        "        urlretrieve(origin, f)\n",
        "    except URLError as e:\n",
        "        raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "    except HTTPError as e:\n",
        "        raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "except (Exception, KeyboardInterrupt) as e:\n",
        "    if os.path.exists(f):\n",
        "        os.remove(f)\n",
        "    raise\n",
        "\n",
        "with tarfile.open(f) as archive:\n",
        "    archive.extractall(\".\")\n",
        "\n",
        "os.remove(f)\n",
        "\n",
        "path = \"BSR/BSDS500/data/images\""
      ],
      "metadata": {
        "id": "t0qpZKA0au-o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BSDS500Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path: str, mode: str, scale: int):\n",
        "        self.path = path\n",
        "        self.mode = mode # train / test / val\n",
        "        self.scale = scale\n",
        "        self.file_names = [os.path.join(self.path, self.mode, file_name) for file_name in os.listdir(os.path.join(self.path, self.mode)) if file_name.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        file_name = self.file_names[index]\n",
        "        image = Image.open(file_name)\n",
        "        image.convert(\"YCbCr\")\n",
        "\n",
        "        low_composed = transforms.Compose([\n",
        "            transforms.Resize([image.size[0] // self.scale, image.size[1] // self.scale]),\n",
        "            transforms.PILToTensor()\n",
        "        ])\n",
        "\n",
        "        high_composed = transforms.Compose([\n",
        "            transforms.PILToTensor()\n",
        "        ])\n",
        "\n",
        "        sample = { \"low\": low_composed(image), \"high\": high_composed(image) }\n",
        "        return sample\n",
        "\n",
        "trainset = BSDS500Dataset(path, \"train\", 3)\n",
        "testset = BSDS500Dataset(path, \"test\", 3)\n",
        "\n",
        "for i in range(len(trainset)):\n",
        "    sample = trainset[i]\n",
        "    print(i, sample[\"low\"].size(), sample[\"high\"].size())\n",
        "    if i == 2:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twct87mDdlsY",
        "outputId": "aaee7d6d-5ec9-4e4b-c1b8-42227de0fc52"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([3, 160, 107]) torch.Size([3, 321, 481])\n",
            "1 torch.Size([3, 107, 160]) torch.Size([3, 481, 321])\n",
            "2 torch.Size([3, 160, 107]) torch.Size([3, 321, 481])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual ESPCN model"
      ],
      "metadata": {
        "id": "KNj2Rq4iIcan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ESPCN_model(nn.Module):\n",
        "    def __init__(self, layer1_channel, layer2_channel, upscale_factor):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, layer1_channel, kernel_size=5, stride=1, padding=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(layer1_channel, layer2_channel, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(layer2_channel, upscale_factor ** 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.PixelShuffle(upscale_factor) # This function is literally build for ESPCN\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "b5--A1VEIc2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for training (adapted from PA2)\n",
        "\n",
        "NOTE: Some of the utilities functions are not updated \n",
        "e.x. we are using BSDS500 rather than CIFAR"
      ],
      "metadata": {
        "id": "zOJ1G8nmHgWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def train(args, cnn=None):\n",
        "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
        "    # torch.set_num_threads(5)\n",
        "\n",
        "    # Numpy random seed\n",
        "    npr.seed(args.seed)\n",
        "\n",
        "    # Save directory\n",
        "    save_dir = \"outputs/\" + args.experiment_name\n",
        "\n",
        "    # INPUT CHANNEL\n",
        "    num_in_channels = 1\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "    if cnn is None:\n",
        "        cnn = ESPCN_model(64, 32, 3)\n",
        "\n",
        "    # LOSS FUNCTION\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(cnn.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4, nesterov=False)\n",
        "\n",
        "    # DATA\n",
        "    print(\"Loading data...\")\n",
        "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
        "\n",
        "    print(\"Transforming data...\")\n",
        "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
        "    train_rgb_cat = get_rgb_cat(train_rgb, colours)\n",
        "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
        "    test_rgb_cat = get_rgb_cat(test_rgb, colours)\n",
        "\n",
        "    # Create the outputs folder if not created already\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(\"Beginning training ...\")\n",
        "    if args.gpu:\n",
        "        cnn.cuda()\n",
        "    start = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        # Train the Model\n",
        "        cnn.train()  # Change model to 'train' mode\n",
        "        losses = []\n",
        "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb_cat, args.batch_size)):\n",
        "            images, labels = get_torch_vars(xs, ys, args.gpu)\n",
        "            # Forward + Backward + Optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn(images)\n",
        "\n",
        "            loss = compute_loss(\n",
        "                criterion, outputs, labels, batch_size=args.batch_size, num_colours=num_colours\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.data.item())\n",
        "\n",
        "        # plot training images\n",
        "        if args.plot:\n",
        "            _, predicted = torch.max(outputs.data, 1, keepdim=True)\n",
        "            plot(\n",
        "                xs,\n",
        "                ys,\n",
        "                predicted.cpu().numpy(),\n",
        "                colours,\n",
        "                save_dir + \"/train_%d.png\" % epoch,\n",
        "                args.visualize,\n",
        "                args.downsize_input,\n",
        "            )\n",
        "\n",
        "        # plot training images\n",
        "        avg_loss = np.mean(losses)\n",
        "        train_losses.append(avg_loss)\n",
        "        time_elapsed = time.time() - start\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Loss: %.4f, Time (s): %d\"\n",
        "            % (epoch + 1, args.epochs, avg_loss, time_elapsed)\n",
        "        )\n",
        "\n",
        "        # Evaluate the model\n",
        "        cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
        "        val_loss, val_acc = run_validation_step(\n",
        "            cnn,\n",
        "            criterion,\n",
        "            test_grey,\n",
        "            test_rgb_cat,\n",
        "            args.batch_size,\n",
        "            colours,\n",
        "            save_dir + \"/test_%d.png\" % epoch,\n",
        "            args.visualize,\n",
        "            args.downsize_input,\n",
        "        )\n",
        "\n",
        "        time_elapsed = time.time() - start\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_accs.append(val_acc)\n",
        "        print(\n",
        "            \"Epoch [%d/%d], Val Loss: %.4f, Val Acc: %.1f%%, Time(s): %.2f\"\n",
        "            % (epoch + 1, args.epochs, val_loss, val_acc, time_elapsed)\n",
        "        )\n",
        "\n",
        "    # Plot training curve\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, \"ro-\", label=\"Train\")\n",
        "    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.savefig(save_dir + \"/training_curve.png\")\n",
        "\n",
        "    if args.checkpoint:\n",
        "        print(\"Saving model...\")\n",
        "        torch.save(cnn.state_dict(), args.checkpoint)\n",
        "\n",
        "    return cnn"
      ],
      "metadata": {
        "cellView": "code",
        "id": "3Ff3lUQzHhJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "2A-HLZ-2Wv29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = AttrDict()\n",
        "args_dict = {\n",
        "    \"gpu\": True,\n",
        "    \"valid\": False,\n",
        "    \"checkpoint\": \"\",\n",
        "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
        "    \"model\": \"UNet\",\n",
        "    \"kernel\": 3,\n",
        "    \"num_filters\": 32,\n",
        "    'learn_rate':0.001, \n",
        "    \"batch_size\": 100,\n",
        "    \"epochs\": 25,\n",
        "    \"seed\": 42,\n",
        "    \"plot\": True,\n",
        "    \"experiment_name\": \"colourization_cnn\",\n",
        "    \"visualize\": False,\n",
        "    \"downsize_input\": False,\n",
        "}\n",
        "args.update(args_dict)\n",
        "cnn = train(args)"
      ],
      "metadata": {
        "id": "hKUcO-YqWxvW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}